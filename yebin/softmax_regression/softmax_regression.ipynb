{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e051c44",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5af6ee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ca084319d0>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5b928",
   "metadata": {},
   "source": [
    "Low Level implementation of Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "742d924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_data = [[1, 2, 3], [3, 5 ,1]]\n",
    "z = torch.FloatTensor(z_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b9f1c80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0900, 0.2447, 0.6652],\n",
      "        [0.1173, 0.8668, 0.0159]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.0000)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 -> 0.09 2 -> 0.2447 3 -> 0.6652 : softmax done\n",
    "hypothesis = F.softmax(z, dim = 1)\n",
    "print(hypothesis)\n",
    "\n",
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "829be95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c3da2a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# z size : (3, 5), dim = 1 -> row-wise softmax, dim =0 -> colun-wise softmax\n",
    "hypothesis = F.softmax(z, dim = 1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "487b6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3, )).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a6d5444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create zero tensor with same size of hypothesis tensor : (3, 5)\n",
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "# give 1 entry to tensor y\n",
    "# y.unsqueeze(1) : (3, ) to (3, 1) \n",
    "# scatter_(1, unsqeeuze, 1) : give entry 1 to row-wise\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "30eca5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [2],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(y.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "17f07dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6c2727e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# hypothesis = F.softmax(z, dim = 1)\n",
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73face",
   "metadata": {},
   "source": [
    "High Level implementation of Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6295deac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.softmax() + torch.log() = F.log_softmax()\n",
    "F.log_softmax(z, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "466cff7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hypothesis = F.softmax(z, dim = 1)\n",
    "# cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e37ce9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or can use\n",
    "# nll : negative log likelihood\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a189398c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or can even use\n",
    "# contains even the softmax function\n",
    "F.cross_entropy(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c129b5a",
   "metadata": {},
   "source": [
    "Implementation of Actual Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "abd1d25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ca084319d0>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0c970dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "x_train = [[1, 2, 1, 1],\n",
    "           [2, 1, 3, 2],\n",
    "           [3, 1, 3, 4],\n",
    "           [4, 1, 5, 5],\n",
    "           [1, 7, 5, 5],\n",
    "           [1, 2, 5, 6],\n",
    "           [1, 6, 6, 6],\n",
    "           [1, 7, 7, 7]]\n",
    "y_train = [2,\n",
    "           2,\n",
    "           2,\n",
    "           1,\n",
    "           1,\n",
    "           1,\n",
    "           0,\n",
    "           0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1355f5a",
   "metadata": {},
   "source": [
    "Low level implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "0b36e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "12a10294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train : 2, 1, 0 -> 3 classes\n",
    "# (8 * 4)_x * (4 * 3)_W = (8 * 3)_y\n",
    "y_one_hot = torch.zeros(8, 3)\n",
    "y_one_hot = y_one_hot.scatter(1, y_train.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "945dba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n",
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y_one_hot.shape)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "7820d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (8 * 4)_x * (4 * 3)_W = (8 * 3)_y\n",
    "W = torch.zeros((4, 3), requires_grad = True)\n",
    "b = torch.zeros((1, 3), requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "453fc55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost : 1.098612\n",
      "Epoch  100/1000 Cost : 0.704199\n",
      "Epoch  200/1000 Cost : 0.622999\n",
      "Epoch  300/1000 Cost : 0.565717\n",
      "Epoch  400/1000 Cost : 0.515291\n",
      "Epoch  500/1000 Cost : 0.467662\n",
      "Epoch  600/1000 Cost : 0.421278\n",
      "Epoch  700/1000 Cost : 0.375402\n",
      "Epoch  800/1000 Cost : 0.329766\n",
      "Epoch  900/1000 Cost : 0.285072\n",
      "Epoch 1000/1000 Cost : 0.248155\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188db74",
   "metadata": {},
   "source": [
    "High level implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8ec67f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost : 1.098612\n",
      "Epoch  100/1000 Cost : 0.704199\n",
      "Epoch  200/1000 Cost : 0.622999\n",
      "Epoch  300/1000 Cost : 0.565717\n",
      "Epoch  400/1000 Cost : 0.515292\n",
      "Epoch  500/1000 Cost : 0.467662\n",
      "Epoch  600/1000 Cost : 0.421278\n",
      "Epoch  700/1000 Cost : 0.375402\n",
      "Epoch  800/1000 Cost : 0.329766\n",
      "Epoch  900/1000 Cost : 0.285073\n",
      "Epoch 1000/1000 Cost : 0.248155\n"
     ]
    }
   ],
   "source": [
    "# Use F.cross_entropy()\n",
    "W = torch.zeros((4,3), requires_grad = True)\n",
    "b = torch.zeros((1,3), requires_grad = True)\n",
    "\n",
    "optimizer = optim.SGD([W, b], lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # F.cross_entropy() already contains softmax : pre-compute xW + b\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2d01e",
   "metadata": {},
   "source": [
    "Implementation using nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2bc2bee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2ca084319d0>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "67880ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247]])), ('bias', tensor([ 0.1826, -0.1949, -0.0365]))])\n",
      "Epoch    0/1000 Cost : 1.616785\n",
      "Epoch  100/1000 Cost : 0.658891\n",
      "Epoch  200/1000 Cost : 0.573443\n",
      "Epoch  300/1000 Cost : 0.518151\n",
      "Epoch  400/1000 Cost : 0.473265\n",
      "Epoch  500/1000 Cost : 0.433516\n",
      "Epoch  600/1000 Cost : 0.396563\n",
      "Epoch  700/1000 Cost : 0.360914\n",
      "Epoch  800/1000 Cost : 0.325392\n",
      "Epoch  900/1000 Cost : 0.289178\n",
      "Epoch 1000/1000 Cost : 0.254148\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(4,3)\n",
    "print(model.state_dict())\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdec1c2",
   "metadata": {},
   "source": [
    "Implementation using class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4081e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 3)\n",
    "        print(self.linear.state_dict())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "e03a70bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2576, -0.2207, -0.0969,  0.2347],\n",
      "        [-0.4707,  0.2999, -0.1029,  0.2544],\n",
      "        [ 0.0695, -0.0612,  0.1387,  0.0247]])), ('bias', tensor([ 0.1826, -0.1949, -0.0365]))])\n"
     ]
    }
   ],
   "source": [
    "model = SoftmaxClassifierModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1d0ee0b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost : 1.616785\n",
      "Epoch  100/1000 Cost : 0.658891\n",
      "Epoch  200/1000 Cost : 0.573443\n",
      "Epoch  300/1000 Cost : 0.518151\n",
      "Epoch  400/1000 Cost : 0.473265\n",
      "Epoch  500/1000 Cost : 0.433516\n",
      "Epoch  600/1000 Cost : 0.396563\n",
      "Epoch  700/1000 Cost : 0.360914\n",
      "Epoch  800/1000 Cost : 0.325392\n",
      "Epoch  900/1000 Cost : 0.289178\n",
      "Epoch 1000/1000 Cost : 0.254148\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    prediction = model(x_train)\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost : {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
